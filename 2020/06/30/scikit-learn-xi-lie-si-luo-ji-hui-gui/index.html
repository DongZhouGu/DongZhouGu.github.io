<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>scikit-learn系列四：逻辑回归 | 欢迎来到，TWOTO 的博客</title><meta name="keywords" content="机器学习"><meta name="author" content="DongZhou"><meta name="copyright" content="DongZhou"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="项目地址传送门，欢迎 star 和 fork ！1. Logistic 回归概述Logistic 回归 或者叫逻辑回归，虽然名字有回归，但是它是用来做分类的。其主要思想是: 根据现有数据对分类边界线 (Decision Boundary) 建立回归公式，以此进行分类。 2. 算法原理假设有一场足球赛，我们有两支球队的所有出场球员信息、历史交锋成绩、比赛时间、主客场、裁判和天气等信息，根据这些信息预">
<meta property="og:type" content="article">
<meta property="og:title" content="scikit-learn系列四：逻辑回归">
<meta property="og:url" content="https://dongzhougu.github.io/2020/06/30/scikit-learn-xi-lie-si-luo-ji-hui-gui/index.html">
<meta property="og:site_name" content="欢迎来到，TWOTO 的博客">
<meta property="og:description" content="项目地址传送门，欢迎 star 和 fork ！1. Logistic 回归概述Logistic 回归 或者叫逻辑回归，虽然名字有回归，但是它是用来做分类的。其主要思想是: 根据现有数据对分类边界线 (Decision Boundary) 建立回归公式，以此进行分类。 2. 算法原理假设有一场足球赛，我们有两支球队的所有出场球员信息、历史交锋成绩、比赛时间、主客场、裁判和天气等信息，根据这些信息预">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-10.png">
<meta property="article:published_time" content="2020-06-30T07:57:44.000Z">
<meta property="article:modified_time" content="2021-11-07T15:49:46.989Z">
<meta property="article:author" content="DongZhou">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-10.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://dongzhougu.github.io/2020/06/30/scikit-learn-xi-lie-si-luo-ji-hui-gui/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: DongZhou","link":"链接: ","source":"来源: 欢迎来到，TWOTO 的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'scikit-learn系列四：逻辑回归',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-11-07 23:49:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    document.addEventListener('pjax:complete', detectApple)})(window)</script><style type="text/css">.card-announcement .social-button{margin: .6rem 0 0 0; text-align: center;}.card-announcement .social-button a{display: block; margin: 0.2rem 0;background-color: var(--btn-bg); color: var(--btn-color); line-height: 1.6rem; transition: all .3s; position: relative; z-index: 1;}</style><style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: #FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }
    
    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }
    
   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}
    
    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}
    
   
    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: #49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: #2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },1000); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
 </script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="欢迎来到，TWOTO 的博客" type="application/atom+xml">
</head>
 <div id="loading-container">
     <p class="loading-text">玩命加载中 . . . </p> 
     <div class="loading-image">
         <div></div>
         <div></div>
         <div></div>
         <div></div> 
         <div></div>
     </div>
 </div><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/DongZhouGu/DongZhouGu.github.io/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">50</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 目录</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-10.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">欢迎来到，TWOTO 的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 目录</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">scikit-learn系列四：逻辑回归<a class="post-edit-link" href="https://github.com/DongZhouGu/Blog-backup/edit/master/source/_posts/scikit-learn系列四：逻辑回归.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2020-06-30T07:57:44.000Z" title="undefined 2020-06-30 15:57:44">2020-06-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/">Python</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="scikit-learn系列四：逻辑回归"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="项目地址传送门，欢迎-star-和-fork-！"><a href="#项目地址传送门，欢迎-star-和-fork-！" class="headerlink" title="项目地址传送门，欢迎 star 和 fork ！"></a>项目地址<a target="_blank" rel="noopener" href="https://github.com/DongZhouGu/scikit-learn-ml">传送门</a>，欢迎 star 和 fork ！</h2><h2 id="1-Logistic-回归概述"><a href="#1-Logistic-回归概述" class="headerlink" title="1. Logistic 回归概述"></a>1. Logistic 回归概述</h2><p>Logistic 回归 或者叫逻辑回归，虽然名字有回归，但是它是用来做分类的。其主要思想是: 根据现有数据对分类边界线 (Decision Boundary) 建立回归公式，以此进行分类。</p>
<h2 id="2-算法原理"><a href="#2-算法原理" class="headerlink" title="2. 算法原理"></a>2. 算法原理</h2><p>假设有一场足球赛，我们有两支球队的所有出场球员信息、历史交锋成绩、比赛时间、主客场、裁判和天气等信息，根据这些信息预测球队的输赢。假设比赛结果记为y，赢球标记为1，输球标记为0，这就是典型的二元分类问题，可以用逻辑回归算法来解决。</p>
<p>与线性回归算法的最大区别是，逻辑回归算法的输出是个离散值。</p>
<h3 id="2-1-预测函数"><a href="#2-1-预测函数" class="headerlink" title="2.1 预测函数"></a>2.1 预测函数</h3><p>需要找出一个预测函数模型，使其值输出在[0,1]之间。然后选择一个基准值，如0.5，如果算出来的预测值大于0.5，就认为其预测值为1，反之，则其预测值为0。</p>
<p>选择Sigmoid函数（也称为Logistic函数，逻辑回归的名字由此而来）<br>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$<br>来作为预测函数，其中e是自然对数的底数。以z为横坐标，以g(z)为纵坐标，画出的图形如下所示：</p>
<p><img src= "/img/loading.gif" data-lazy-src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/dongzhougu/imageuse1/LR_3.png" alt="Sigmoid 函数在不同坐标下的图片"></p>
<p>从图中可以看出，当z=0时，g(z)=0.5；当z&gt;0时，g(z)&gt;0.5，当z越来越大时，g(z)无限接近于1；当z&lt;0时，g(z)&lt;0.5，当z越来越小时，g(z)无限接近于0。这正是我们想要的针对二元分类算法的预测函数。</p>
<h3 id="2-2-判定边界"><a href="#2-2-判定边界" class="headerlink" title="2.2 判定边界"></a>2.2 判定边界</h3><p>逻辑回归算法的预测函数由下面两个公式给出：<br>$$<br>h_{\theta}(x)=g\left(\theta^{T} x\right)<br>$$</p>
<p>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$</p>
<p>下面给出两个判定边界的例子。假设有两个变量x1，x2，其逻辑回归预测函数是$h_{\theta}(x)=g\left(\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}\right)$</p>
<p>假设给定参数：<br>$$<br>\theta=\left[\begin{array}{c}<br>-3 \<br>1 \<br>1<br>\end{array}\right]<br>$$<br>那么，可以得到判定边$-3+x_{1}+x_{2}=0$ ，如果以 $x_{1}$ 为横坐标， $x_{2}$  为纵坐标，则这个函数画出来就是一条通过(0,3)和(3,0)两点的直线。这条线就是判定边界，其中，直线左下方为y=0，直线右上方为y=1，如图所示：</p>
<p><img src= "/img/loading.gif" data-lazy-src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/dongzhougu/imageuse1/17634123-0bf913a36c2847a8.png" alt="img"></p>
<p>如果预测函数是多项式 $h_{\theta}(x)=g\left(\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\theta_{3} x_{1}^{2}+\theta_{4} x_{2}^{2}\right)$，且给定</p>
<p>$$<br>\theta=\left[\begin{array}{c}<br>-1 \<br>0 \<br>0 \<br>1 \<br>1<br>\end{array}\right]<br>$$<br>则可以得到判定边界函数$x_{1}^{2}+x_{2}^{2}=1$ 则这是一个半径为1的圆。圆内部是y=0，圆外部是y=1，如上图所示。</p>
<h3 id="2-3-损失函数"><a href="#2-3-损失函数" class="headerlink" title="2.3 损失函数"></a>2.3 损失函数</h3><p>我们不能使用线性回归模型的损失函数来推导逻辑回归的损失函数，因为那样的损失函数太复杂，最终很可能会导致无法通过迭代找到损失函数值最小的点。</p>
<p>为了容易地求出损失函数的最小值，我们分成 y=1 和 y=0 两种情况来分别考虑其预测值和真实值的误差。我们先考虑最简单的情况，即计算某个样本 x，y=1 和 y=0 两种情况下的预测值与真实值的误差，我们选择的损失公式如下：</p>
<p>$\operatorname{cost}\left(h_{\theta}(x), y\right)=\left{\begin{array}{ccc}-\log \left(h_{\theta}(x)\right), &amp; \text { if } &amp; y=1 \ -\log \left(1-h_{\theta}(x)\right), &amp; \text { if } &amp; y=0\end{array}\right.$</p>
<p>其中， $h_{\theta}(x)$ 表示预测为1的概率，log(x)为自然对数。如图所示</p>
<p><img src= "/img/loading.gif" data-lazy-src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/dongzhougu/imageuse1/image-20200630141514106.png" alt="img"></p>
<p>根据损失函数的定义，损失是预测值与真实值的差异。当差异越大时，损失越大，模型受到的“惩罚”也越严重。在左图中，当 y=1 时，随着（预测为1的概率）越来越大，预测值越来越接近真实值，其损失越来越小；在右图中，当 y=0 时，随着（预测为1的概率）越来越大，预测值越来越偏离真实值，其损失越来越大。</p>
<h3 id="2-4-梯度下降算法"><a href="#2-4-梯度下降算法" class="headerlink" title="2.4 梯度下降算法"></a>2.4 梯度下降算法</h3><p>和线性回归类似，这里使用梯度下降算法来求解逻辑回归模型参数。具体可见上一节 <a href="https://dongzhougu.github.io/2020/06/30/scikit-learn-xi-lie-san-xian-xing-hui-gui/">线性回归回归算法</a>。</p>
<h2 id="3-多元分类"><a href="#3-多元分类" class="headerlink" title="3. 多元分类"></a>3. 多元分类</h2><p>逻辑回归模型可以解决二元分类问题，即 y={0,1}，能不能解决多元分类问题呢？答案是肯定的。针对多元分类问题，y={0,1,2,3,…,n}，总共有n+1个类别。其解决思路是：首先把问题转换为二元分类问题，即y=0是一个类别，y={1,2,3,…,n}作为另外一个类别，然后计算这两个类别的概率；接着，把y=1作为一个类别，把y={0,2,3,…,n}作为另外一个类别，再计算这两个类别的概率。</p>
<h2 id="4-正则化"><a href="#4-正则化" class="headerlink" title="4. 正则化"></a>4. 正则化</h2><p>我们知道，过拟合是指模型很好地拟合了训练样本，但对新数据预测的准确性很差，这是因为模型太复杂了。解决办法是减少输入特征的个数，或者获取更多的训练样本。这里介绍的正则化也可以用来解决过拟合问题：</p>
<ul>
<li><p>保留所有的特征，减少特征的权重 $\theta_{j} $ 的值。确保所有的特征对预测值都有少量的贡献。</p>
</li>
<li><p>当每个特征 $x_{j} $ 对预测值y都有少量的贡献时，这样的模型可以良好的工作，这正是正则化的目的，可以用它来解决特征过多时的过拟合问题。</p>
</li>
</ul>
<h3 id="4-1-线性回归模型正则化"><a href="#4-1-线性回归模型正则化" class="headerlink" title="4.1 线性回归模型正则化"></a>4.1 线性回归模型正则化</h3><p>我们先来看线性回归模型的损失函数是如何正则化的：<br>$$<br>J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}<br>$$<br>公式中前半部分就是原来的线性回归模型的损失函数，也称为预测值与实际值的误差。后半部分为加入的正则项。其中 $\lambda $ 的值有两个目的，即要维持对训练样本的拟合，又要避免对训练样本的过拟合。如果  $\lambda $  的值太大，则能确保不出现过拟合，但可能会导致对现有训练样本出现欠拟合。</p>
<h3 id="4-2-线性回归模型正则化"><a href="#4-2-线性回归模型正则化" class="headerlink" title="4.2 线性回归模型正则化"></a>4.2 线性回归模型正则化</h3><p>同样，可以对逻辑回归模型的损失函数进行正则化，其方法也是在原来的损失函数的基础上加上正则项：<br>$$<br>J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}<br>$$</p>
<h2 id="5-算法参数"><a href="#5-算法参数" class="headerlink" title="5. 算法参数"></a>5. 算法参数</h2><p>在 <code>scikit-learn</code>里，逻辑回归模型由类 <code>sklearn.linear_model.LogisticRegression</code>实现。</p>
<h3 id="5-1-正则项权重"><a href="#5-1-正则项权重" class="headerlink" title="5.1 正则项权重"></a>5.1 正则项权重</h3><p>上面介绍的正则项权重  $\lambda $  ，在<code>LogisticRegression</code>类里有个参数 C 与之对应，但成反比。即 C 值越大，  $\lambda $ 越小，模型容易出现过拟合；C 值越小，  $\lambda $  越大，模型容易出现欠拟合。</p>
<h3 id="5-2-L1-L2范数"><a href="#5-2-L1-L2范数" class="headerlink" title="5.2 L1/L2范数"></a>5.2 L1/L2范数</h3><p>创建逻辑回归模型时，有个参数penalty（惩罚），其取值有“l1”或“l2”</p>
<ul>
<li>L1范数作为正则项，会让模型参数 $\theta$ 稀疏化，即让模型参数向量里的0元素尽可能多，只保留模型参数向量中重要特征的贡献。</li>
<li>L2范数作为正则项，则让模型参数尽量小，但不会为0，即尽量让每个特征对应预测值都有一些小的贡献。</li>
</ul>
<p>假设模型只有两个参数，它们构成一个二维向量 $\theta=\left[\theta_{1}, \theta_{2}\right]$,则L1范数为：</p>
<p>$|\theta|<em>{1}=\left|\theta</em>{1}\right|+\left|\theta_{2}\right|$</p>
<p>即L1范数是向量里元素的绝对值之和。L2范数为向量里所有元素的平方和的算术平方根：</p>
<p>$|\theta|<em>{2}=\sqrt{\theta</em>{1}^{2}+\theta_{2}^{2}}$</p>
<p>我们知道，梯度下降算法在参数迭代的过程中，实际上是在损失函数的等高线上跳跃，并最终收敛在误差最小的点上。那么正则项的本质是什么？正则项的本质是惩罚。在参数迭代的过程中，如果没有遵循正则项所表达的规则，那么其损失会变大，即受到了惩罚，从而往正则项所表达的规则处收敛。正则化后的模型参数应该收敛在误差等高线与正则项等高线相切的点上。</p>
<p>作为推论，L1范数作为正则项，有以下几个用途：</p>
<ul>
<li>选择重要特征：L1范数会让模型参数向量里的元素为0的点尽量多，这样可以排除掉那些对预测值没有什么影响的特征，从而简化问题。所以L1范数解决过拟合，实际上是减少特征数量。</li>
<li>模型可解释性好：模型参数向量稀疏化后，只会留下那些对预测值有重要影响的特征。这样我们就容易解释模型的因果关系。比如，针对某种癌症的筛查，如果有100个特征，那么我们无从解释到底哪些特征对阳性呈关键作用。稀疏化后，只留下几个关键的特征，就容易看到因果关系。</li>
</ul>
<p>由此可见，L1范数作为正则项，更多的是一个分析工具，而适合用来对模型求解。因为它会把不重要的特征直接去除。大部分的情况下解决过拟合问题，还是选择L2范数作为正则项，这也是 <code>scikit-learn</code> 里的默认值。</p>
<h2 id="6-示例：乳腺癌检测"><a href="#6-示例：乳腺癌检测" class="headerlink" title="6 示例：乳腺癌检测"></a>6 示例：乳腺癌检测</h2><p>本节来看一个实例，使用逻辑回归算法解决乳腺癌检测问题。我们需要先采集肿瘤病灶造影图片，然后对图片进行分析，从图片中提取特征，再根据特征来训练模型。最终使用模型来检测新采集到的肿瘤病灶造影，以便判断肿瘤是良性的还是恶性的。这是个典型的二元分类问题。</p>
<h3 id="6-1-数据采集及特征提取"><a href="#6-1-数据采集及特征提取" class="headerlink" title="6.1 数据采集及特征提取"></a>6.1 数据采集及特征提取</h3><p>为了简单起见，直接加载 <code>scikit-learn</code> 自带的一个乳腺癌数据集。这个数据集是已经采集后的数据：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X = cancer.data</span><br><span class="line">y = cancer.target</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'data shape: {0}; no. positive: {1}; no. negative: {2}'</span></span><br><span class="line">      .<span class="built_in">format</span>(X.shape,y[y==<span class="number">1</span>].shape[<span class="number">0</span>],y[y==<span class="number">0</span>].shape[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(cancer.data[<span class="number">0</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>输出如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data shape: (<span class="number">569</span>, <span class="number">30</span>); no. positive: <span class="number">357</span>; no. negative: <span class="number">212</span></span><br><span class="line">[<span class="number">1.799e+01</span> <span class="number">1.038e+01</span> <span class="number">1.228e+02</span> <span class="number">1.001e+03</span> <span class="number">1.184e-01</span> <span class="number">2.776e-01</span> <span class="number">3.001e-01</span></span><br><span class="line"> <span class="number">1.471e-01</span> <span class="number">2.419e-01</span> <span class="number">7.871e-02</span> <span class="number">1.095e+00</span> <span class="number">9.053e-01</span> <span class="number">8.589e+00</span> <span class="number">1.534e+02</span></span><br><span class="line"> <span class="number">6.399e-03</span> <span class="number">4.904e-02</span> <span class="number">5.373e-02</span> <span class="number">1.587e-02</span> <span class="number">3.003e-02</span> <span class="number">6.193e-03</span> <span class="number">2.538e+01</span></span><br><span class="line"> <span class="number">1.733e+01</span> <span class="number">1.846e+02</span> <span class="number">2.019e+03</span> <span class="number">1.622e-01</span> <span class="number">6.656e-01</span> <span class="number">7.119e-01</span> <span class="number">2.654e-01</span></span><br><span class="line"> <span class="number">4.601e-01</span> <span class="number">1.189e-01</span>]</span><br></pre></td></tr></tbody></table></figure>

<p>数据集中总共有569个样本，每个样本有30个特征，其中357个阳性（y=1）样本，212个阴性（y=0）样本。同时，还打印出一个样本数据，以便直观地进行观察。</p>
<p>这30个特征是怎么来的呢？这个数据集总共从病灶造影图片中提取了以下10个关键属性：</p>
<ul>
<li>radius：半径，即病灶中心点离边界的平均距离。</li>
<li>texture：纹理，灰度值的标准偏差。</li>
<li>perimeter：周长，即病灶的大小。</li>
<li>area：面积，也是反映病灶大小的一个指标。</li>
<li>smoothness：平滑度，即半径的变化幅度。</li>
<li>compactness：密实度，周长的平方除以面积，再减去1</li>
<li>concavity：凹度，凹陷部分轮廓的严重程度。</li>
<li>concave points：凹点，凹陷轮廓的数量。</li>
<li>symmetry：对称性。</li>
<li>fractal demension：分形维度。</li>
</ul>
<p>实际上它只关注10个特征，然后又构造出了每个特征的标准差及最大值，这样每个特征就衍生出了两个特征，所以总共就有了30个特征。可以通过 <code>cancer.feature_names</code> 变量来查看这些特征的名称。</p>
<h3 id="6-2-模型训练"><a href="#6-2-模型训练" class="headerlink" title="6.2 模型训练"></a>6.2 模型训练</h3><p>首先，把数据集分成训练数据集和测试数据集：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>然后使用 <code>LogisticRegression</code> 模型来训练，并计算训练数据集的评分数据和测试数据集的评分数据：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train,y_train)</span><br><span class="line">train_score = model.score(X_train,y_train)</span><br><span class="line">test_score = model.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'train score: {train_score:.6f}; test_score:{test_score:.6f}'</span></span><br><span class="line">      .<span class="built_in">format</span>(train_score=train_score,</span><br><span class="line">             test_score=test_score))</span><br></pre></td></tr></tbody></table></figure>

<p>输出如下：</p>
<figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train score: <span class="number">0.940659</span>; test_score:<span class="number">0.964912</span></span><br></pre></td></tr></tbody></table></figure>

<p>观察模型在测试样本集的表现：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'matchs: {0}/{1}'</span>.<span class="built_in">format</span>(np.equal(y_pred,y_test).shape[<span class="number">0</span>],y_test.shape[<span class="number">0</span>]))</span><br></pre></td></tr></tbody></table></figure>

<p>输出如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matchs: <span class="number">114</span>/<span class="number">114</span></span><br></pre></td></tr></tbody></table></figure>

<p>总共114个测试样本，全部预测正确。为什么 <code>testscore</code> 却只有0.973684，而不是1呢？答案是，<code>scikit-learn</code>不是使用这个数据来计算分数，因为这个数据不能完全反映误差情况，而是使用预测概率数据计算模型评分。</p>
<p>针对二元分类问题，<code>LogisticRegression</code>模型会对每个样本输出两个概率，即为 0 的概率和为 1 的概率，哪个概率高就预测为哪个类别。</p>
<p>找出测试数据集中预测“自信度”低于90%的样本。这里先计算出测试数据集里的每个样本的预测概率数据，针对每个样本，它会有两个数据，一是预测其为阳性的概率，另外一个是预测其为阴性的概率。接着找出预测为阴性的概率大于0.1且小于0.9的样本（同时也是预测为阳性的概率大于0.1小于0.9），这些样本就是“自信度”不足90%的样本。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测概率：找出预测概率低于 90% 的样本</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)  <span class="comment"># 计算每个测试样本的预测概率</span></span><br><span class="line"><span class="comment"># 找出第一列，即预测为阴性的概率大于 0.1 的样本，保存在 result 里</span></span><br><span class="line">y_pred_proba_0 = y_pred_proba[:, <span class="number">0</span>] &gt; <span class="number">0.1</span></span><br><span class="line">result = y_pred_proba[y_pred_proba_0]</span><br><span class="line"><span class="comment"># 在 result 结果集里，找出第二列，即预测为阳性的概率大于 0.1 的样本</span></span><br><span class="line">y_pred_proba_1 = result[:, <span class="number">1</span>] &gt; <span class="number">0.1</span></span><br><span class="line"><span class="built_in">print</span>(result[y_pred_proba_1])</span><br></pre></td></tr></tbody></table></figure>

<p>输出如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.29623162</span> <span class="number">0.70376838</span>]</span><br><span class="line"> [<span class="number">0.54660262</span> <span class="number">0.45339738</span>]</span><br><span class="line"> [<span class="number">0.17874247</span> <span class="number">0.82125753</span>]</span><br><span class="line"> [<span class="number">0.20917573</span> <span class="number">0.79082427</span>]</span><br><span class="line"> [<span class="number">0.10943452</span> <span class="number">0.89056548</span>]</span><br><span class="line"> [<span class="number">0.35503614</span> <span class="number">0.64496386</span>]</span><br><span class="line"> [<span class="number">0.23849987</span> <span class="number">0.76150013</span>]</span><br><span class="line"> [<span class="number">0.13634228</span> <span class="number">0.86365772</span>]</span><br><span class="line"> [<span class="number">0.80171734</span> <span class="number">0.19828266</span>]</span><br><span class="line"> [<span class="number">0.21744759</span> <span class="number">0.78255241</span>]</span><br><span class="line"> [<span class="number">0.81346356</span> <span class="number">0.18653644</span>]</span><br><span class="line"> [<span class="number">0.2225791</span>  <span class="number">0.7774209</span> ]</span><br><span class="line"> [<span class="number">0.10788007</span> <span class="number">0.89211993</span>]</span><br><span class="line"> [<span class="number">0.88068005</span> <span class="number">0.11931995</span>]</span><br><span class="line"> [<span class="number">0.18189724</span> <span class="number">0.81810276</span>]]</span><br></pre></td></tr></tbody></table></figure>

<p>由此可见，计算预测概率使用model.predict_proba()函数，而计算预测分类用model.predict()函数。</p>
<h3 id="6-3-模型优化"><a href="#6-3-模型优化" class="headerlink" title="6.3 模型优化"></a>6.3 模型优化</h3><p>首先，使用Pipeline来增加多项式特征：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加多项式预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polynomial_model</span>(<span class="params">degree=<span class="number">1</span>, **kwarg</span>):</span></span><br><span class="line">    polynomial_features = PolynomialFeatures(degree=degree,</span><br><span class="line">                                             include_bias=<span class="literal">False</span>)</span><br><span class="line">    logistic_regression = LogisticRegression(**kwarg)</span><br><span class="line">    pipeline = Pipeline([(<span class="string">"polynomial_features"</span>, polynomial_features),</span><br><span class="line">                         (<span class="string">"logistic_regression"</span>, logistic_regression)])</span><br><span class="line">    <span class="keyword">return</span> pipeline</span><br></pre></td></tr></tbody></table></figure>

<p>接着，增加二阶多项式特征，创建并训练模型：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">model = polynomial_model(degree=<span class="number">2</span>, penalty=<span class="string">'l1'</span>, solver=<span class="string">'liblinear'</span>)</span><br><span class="line">start = time.process_time()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line">train_score = model.score(X_train, y_train)</span><br><span class="line">test_score = model.score(X_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'elaspe: {0:.6f}; train_score: {1:0.6f}; cv_score: {2:.6f}'</span>.<span class="built_in">format</span>(</span><br><span class="line">    time.process_time() - start, train_score, test_score))</span><br></pre></td></tr></tbody></table></figure>

<p>使用L1范数作为正则项（参数penalty=’l1’），输出如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">elaspe: <span class="number">0.156250</span>; train_score: <span class="number">1.000000</span>; cv_score: <span class="number">0.956140</span></span><br></pre></td></tr></tbody></table></figure>

<p>可以看到，训练数据集评分和测试数据集评分都增加了。为什么使用L1范数作为正则项呢？L1范数作为正则项可以实现参数的稀疏化，即自动选择出那些对模型有关联的重要特征。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">logistic_regression = model.named_steps[<span class="string">'logistic_regression'</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'model parameters shape: {0}; count of non-zero element: {1}'</span>.<span class="built_in">format</span>(</span><br><span class="line">    logistic_regression.coef_.shape, </span><br><span class="line">    np.count_nonzero(logistic_regression.coef_)))</span><br></pre></td></tr></tbody></table></figure>

<p>输出如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model parameters shape: (<span class="number">1</span>, <span class="number">495</span>); count of non-zero element: <span class="number">110</span></span><br></pre></td></tr></tbody></table></figure>

<p>逻辑回归模型的coef_属性里保存的就是模型参数。从输出结果可以看到，增加二阶多项式特征后，输入特征由原来的30个增加到了495个，最终大多数特征都被丢弃，只保留了110个有效特征。</p>
<h3 id="6-4-学习曲线"><a href="#6-4-学习曲线" class="headerlink" title="6.4 学习曲线"></a>6.4 学习曲线</h3><p>首先画出使用L1范数作为正则项所对应的一阶和二阶多项式的学习曲线：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> plot_learning_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=<span class="number">10</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">title = <span class="string">'Learning Curves (degree={0}, penalty={1})'</span></span><br><span class="line">degrees = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">penalty = <span class="string">'l1'</span></span><br><span class="line"></span><br><span class="line">start = time.process_time()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">4</span>), dpi=<span class="number">144</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="built_in">len</span>(degrees), i + <span class="number">1</span>)</span><br><span class="line">    plot_learning_curve(plt, polynomial_model(degree=degrees[i], penalty=penalty, solver=<span class="string">'liblinear'</span>, max_iter=<span class="number">300</span>), </span><br><span class="line">                        title.<span class="built_in">format</span>(degrees[i], penalty), X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.01</span>), cv=cv)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'elaspe: {0:.6f}'</span>.<span class="built_in">format</span>(time.process_time()-start))</span><br></pre></td></tr></tbody></table></figure>

<p>输出的结果如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l1_elaspe: <span class="number">10.781250</span></span><br></pre></td></tr></tbody></table></figure>

<p>L1范数学习曲线如下图所示：</p>
<p><img src= "/img/loading.gif" data-lazy-src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/dongzhougu/imageuse1/17634123-08d8944e88c2ae64.png" alt="image-20200630141514106"></p>
<p>接着画出使用L2范数作为正则项所对应的一阶和二阶多项式的学习曲线：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line">penalty = <span class="string">'l2'</span></span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">4</span>), dpi=<span class="number">144</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(degrees)):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="built_in">len</span>(degrees), i + <span class="number">1</span>)</span><br><span class="line">    plot_learning_curve(plt, polynomial_model(degree=degrees[i], penalty=penalty, solver=<span class="string">'lbfgs'</span>), </span><br><span class="line">                        title.<span class="built_in">format</span>(degrees[i], penalty), X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.01</span>), cv=cv)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'elaspe: {0:.6f}'</span>.<span class="built_in">format</span>(time.clock()-start))</span><br></pre></td></tr></tbody></table></figure>

<p>输出的结果如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l2_elaspe: <span class="number">2.718750</span></span><br></pre></td></tr></tbody></table></figure>

<p>L2范数学习曲线如下图所示：</p>
<p><img src= "/img/loading.gif" data-lazy-src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/dongzhougu/imageuse1/image-20200630141743010.png" alt="image-20200630141743010"></p>
<p>可以明显地看出，使用二阶多项式并使用L1范数作为正则项的模型最优，因为它的训练样本评分最高，交叉验证样本评分也最高。从图中还可以看出，训练样本评分和交叉验证样本评分之间的间隙还比较大，我们可以采集更多的数据来训练模型，以便进一步优化模型。</p>
<p>另外从输出的时间可以看出，L1 范数对应的学习曲线，需要花费较长的时间，原因是，<code>scikit-learn</code> 的<code>learning_curve()</code> 函数在画学习曲线的过程中，要对模型进行多次训练，并计算交叉验证样本评分。同时，为了使曲线更平滑，针对每个点还会进行多次计算求平均值。这个就是 <code>ShuffleSplit</code> 类的作用。在我们这个实例里，只有569个训练样本，这是个很小的数据集。如果数据集增加100倍，甚至1000倍，拿出来画学习曲线将是场灾难。</p>
<p>那么，针对大数据集，怎样高效地画学习曲线？答案很简单，可以从大数据集里选择一小部分数据来画学习曲线，待选择好最优的模型之后，再使用全部的数据集来训练模型。但是要尽量保持选择出来的这部分数据的标签分布与大数据集的标签分布相同，如针对二元分类，阳性和阴性比例要一致。更直观的说就是，抽取出来的样本集为原来数据集的一个缩影，尽可能相似。</p>
<h2 id="7-拓展阅读"><a href="#7-拓展阅读" class="headerlink" title="7.拓展阅读"></a>7.拓展阅读</h2><p>实际上，我们的预测函数就是写成向量形式的：<br>$$<br>h_{\theta}(x)=g(z)=g\left(\theta^{T} x\right)=\frac{1}{1+e^{-\theta^{T} x}}<br>$$<br>这个预测函数一次只计算一个训练样本的预测值，怎样一次性计算出所有样本的预测值呢？答案是把预测函数的参数写成向量的形式：<br>$$<br>h=g(X \theta)<br>$$<br>其中g(x)为Sigmoid函数。X为m×n的矩阵，即数据集的矩阵表达。损失函数也有对应的矩阵形式：<br>$$<br>J(\theta)=\frac{1}{m}\left(-y^{T} \log (h)-(1-y)^{T} \log (1-h)\right)<br>$$<br>其中，y为目标值向量，h为一次性计算出来的所有样本的预测值。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">DongZhou</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://dongzhougu.github.io/2020/06/30/scikit-learn-xi-lie-si-luo-ji-hui-gui/">https://dongzhougu.github.io/2020/06/30/scikit-learn-xi-lie-si-luo-ji-hui-gui/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://dongzhougu.github.io" target="_blank">欢迎来到，TWOTO 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-10.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/DongZhouGu/DongZhouGu.github.io/img/wechat.png" target="_blank"><img class="post-qr-code-img" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/DongZhouGu/DongZhouGu.github.io/img/wechat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/DongZhouGu/DongZhouGu.github.io/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/DongZhouGu/DongZhouGu.github.io/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><meta name="referrer" content="no-referrer" /><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/06/30/scikit-learn-xi-lie-wu-jue-ce-shu/"><img class="prev-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-6.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/DongZhouGu/DongZhouGu.github.io/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">scikit-learn系列五：决策树</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/30/scikit-learn-xi-lie-san-xian-xing-hui-gui/"><img class="next-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-10.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/DongZhouGu/DongZhouGu.github.io/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">scikit-learn系列三：线性回归</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/07/16/ji-qi-xue-xi-suan-fa-shi-xian/" title="机器学习算法实现"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-16</div><div class="title">机器学习算法实现</div></div></a></div><div><a href="/2020/06/30/scikit-learn-xi-lie-san-xian-xing-hui-gui/" title="scikit-learn系列三：线性回归"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-10.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-30</div><div class="title">scikit-learn系列三：线性回归</div></div></a></div><div><a href="/2020/06/28/scikit-learn-xi-lie-yi-ji-qi-xue-xi-ji-chu/" title="scikit-learn系列一：机器学习基础"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-28</div><div class="title">scikit-learn系列一：机器学习基础</div></div></a></div><div><a href="/2020/07/16/scikit-learn-xi-lie-jiu-k-jun-zhi/" title="scikit-learn系列九：K-均值"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-16</div><div class="title">scikit-learn系列九：K-均值</div></div></a></div><div><a href="/2020/06/29/scikit-learn-xi-lie-er-k-jin-lin/" title="scikit-learn系列二：K-近邻"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-8.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-29</div><div class="title">scikit-learn系列二：K-近邻</div></div></a></div><div><a href="/2020/06/30/scikit-learn-xi-lie-wu-jue-ce-shu/" title="scikit-learn系列五：决策树"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-6.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-30</div><div class="title">scikit-learn系列五：决策树</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/DongZhouGu/DongZhouGu.github.io/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">DongZhou</div><div class="author-info__description">技术、效率、摄影</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">50</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/DongZhouGu"><i class="fab fa-github"></i><span>GitHub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1596586942&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="https://github.com/DongZhouGu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/gdz678@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E5%9C%B0%E5%9D%80%E4%BC%A0%E9%80%81%E9%97%A8%EF%BC%8C%E6%AC%A2%E8%BF%8E-star-%E5%92%8C-fork-%EF%BC%81"><span class="toc-number">1.</span> <span class="toc-text">项目地址传送门，欢迎 star 和 fork ！</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Logistic-%E5%9B%9E%E5%BD%92%E6%A6%82%E8%BF%B0"><span class="toc-number">2.</span> <span class="toc-text">1. Logistic 回归概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">2. 算法原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 预测函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 判定边界</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">3.4.</span> <span class="toc-text">2.4 梯度下降算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB"><span class="toc-number">4.</span> <span class="toc-text">3. 多元分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">4. 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 线性回归模型正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 线性回归模型正则化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%AE%97%E6%B3%95%E5%8F%82%E6%95%B0"><span class="toc-number">6.</span> <span class="toc-text">5. 算法参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%AD%A3%E5%88%99%E9%A1%B9%E6%9D%83%E9%87%8D"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 正则项权重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-L1-L2%E8%8C%83%E6%95%B0"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 L1&#x2F;L2范数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%A4%BA%E4%BE%8B%EF%BC%9A%E4%B9%B3%E8%85%BA%E7%99%8C%E6%A3%80%E6%B5%8B"><span class="toc-number">7.</span> <span class="toc-text">6 示例：乳腺癌检测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%8F%8A%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">7.1.</span> <span class="toc-text">6.1 数据采集及特征提取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">7.2.</span> <span class="toc-text">6.2 模型训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96"><span class="toc-number">7.3.</span> <span class="toc-text">6.3 模型优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF"><span class="toc-number">7.4.</span> <span class="toc-text">6.4 学习曲线</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB"><span class="toc-number">8.</span> <span class="toc-text">7.拓展阅读</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/11/15/yuque/springboot-zheng-he-redis/" title="SpringBoot-整合Redis">SpringBoot-整合Redis</a><time datetime="2021-11-15T11:11:53.000Z" title="发表于 2021-11-15 19:11:53">2021-11-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/11/15/yuque/fen-bu-shi-xiao-xi-zhong-jian-jian-gai-shu/" title="分布式消息中间件-概述">分布式消息中间件-概述</a><time datetime="2021-11-15T10:58:22.000Z" title="发表于 2021-11-15 18:58:22">2021-11-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/11/15/yuque/springboot-qian-hou-duan-fen-chi-kua-yu-pei-zhi/" title="SpringBoot-前后端分离跨域配置">SpringBoot-前后端分离跨域配置</a><time datetime="2021-11-15T06:57:12.000Z" title="发表于 2021-11-15 14:57:12">2021-11-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/11/14/yuque/mybatis-jia-gou-yu-yuan-li/" title="MyBatis-架构与原理">MyBatis-架构与原理</a><time datetime="2021-11-14T04:59:00.000Z" title="发表于 2021-11-14 12:59:00">2021-11-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/11/14/yuque/mybatis-huan-cun-gong-zuo-yuan-li/" title="Mybatis-缓存工作原理">Mybatis-缓存工作原理</a><time datetime="2021-11-14T04:58:08.000Z" title="发表于 2021-11-14 12:58:08">2021-11-14</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-10.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By DongZhou</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@3.8.4/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@3.8.4/source/js/main.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly.js.org/source/selfjs/tw_cn.min.js?ver=3.8.4"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly.js.org/source/selfjs/localsearch.min.js?ver=3.8.4"></script><div class="js-pjax"><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      true && mermaid.init()
    })
  }
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'RPCuj0HNm1eqAREO6c5T7nSJ-gzGzoHsz',
      appKey: 'laCdQbWLFWOWdkXVM3RxoXGe',
      placeholder: '记得留下你的昵称和邮箱....可以快速收到回复',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, {"path":"/messageboard/"}))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script>(function(d, w, c) {
    w.ChatraID = 'ZjXyfadNfHZ8yJGFm';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (true) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
  }
  chatBtnFn()
} else {
  if (true) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/talking/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.removeEventListener('scroll', window.tocScrollFn)
  window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,e,a=0;a<r.length;a++)t=r[a],e=void 0,0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body></html>